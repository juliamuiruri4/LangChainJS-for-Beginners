/**
 * Embedding Relationships - Vector Math Demo
 *
 * This example demonstrates how embeddings capture semantic relationships
 * that can be manipulated through vector arithmetic.
 *
 * Run: tsx 04-documents-embeddings-semantic-search/code/09-embedding-relationships.ts
 */

import { createEmbeddingsModel } from "@/scripts/create-model.js";
import "dotenv/config";

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(vecA: number[], vecB: number[]): number {
  const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
  const magnitudeA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
  const magnitudeB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
  return dotProduct / (magnitudeA * magnitudeB);
}

// Helper function to subtract two vectors
function subtractVectors(vecA: number[], vecB: number[]): number[] {
  return vecA.map((a, i) => a - vecB[i]);
}

// Helper function to add two vectors
function addVectors(vecA: number[], vecB: number[]): number[] {
  return vecA.map((a, i) => a + vecB[i]);
}

async function main() {
  console.log("üî¨ Embedding Relationships: Vector Math Demo\n");
  console.log("This demonstrates how embeddings capture semantic relationships");
  console.log("that can be manipulated mathematically.\n");
  console.log("=".repeat(70) + "\n");

  // Initialize embeddings model
  const embeddings = createEmbeddingsModel();

  // ============================================================================
  // Example 1: Geography Relationships
  // Demonstrating: Paris - France + Italy ‚âà Rome
  // ============================================================================

  console.log("üìç Example 1: Geography Relationships");
  console.log("‚îÄ".repeat(70));
  console.log("\nTesting: Embedding('Paris') - Embedding('France') + Embedding('Italy')");
  console.log("Expected result: Should be similar to Embedding('Rome')\n");

  // Generate embeddings for geographic terms
  const [parisEmbed, franceEmbed, italyEmbed, romeEmbed] = await Promise.all([
    embeddings.embedQuery("Paris"),
    embeddings.embedQuery("France"),
    embeddings.embedQuery("Italy"),
    embeddings.embedQuery("Rome"),
  ]);

  // Perform vector arithmetic: Paris - France + Italy
  const parisMinusFrance = subtractVectors(parisEmbed, franceEmbed);
  const result1 = addVectors(parisMinusFrance, italyEmbed);

  // Calculate similarity with Rome
  const similarityToRome = cosineSimilarity(result1, romeEmbed);

  console.log(`‚úÖ Similarity to 'Rome': ${(similarityToRome * 100).toFixed(2)}%`);
  console.log("\nWhat this means:");
  console.log("  ‚Ä¢ Paris is to France as Rome is to Italy");
  console.log("  ‚Ä¢ The vectors encode 'capital city' and 'country' as separate dimensions");
  console.log("  ‚Ä¢ Vector math preserves these relationships!");

  // Show comparison with unrelated terms
  const londonEmbed = await embeddings.embedQuery("London");
  const similarityToLondon = cosineSimilarity(result1, londonEmbed);

  console.log(`\nüìä Comparison: Similarity to 'London': ${(similarityToLondon * 100).toFixed(2)}%`);
  console.log(`   (Lower than Rome, as expected - London is capital of UK, not Italy)\n`);

  console.log("=".repeat(70) + "\n");

  // ============================================================================
  // Example 2: Cultural Food Relationships
  // Demonstrating: pizza - Italy + Japan ‚âà sushi
  // ============================================================================

  console.log("üçï Example 2: Cultural Food Relationships");
  console.log("‚îÄ".repeat(70));
  console.log("\nTesting: Embedding('pizza') - Embedding('Italy') + Embedding('Japan')");
  console.log("Expected result: Should be similar to Embedding('sushi')\n");

  // Generate embeddings for food and countries
  const [pizzaEmbed, japanEmbed, sushiEmbed] = await Promise.all([
    embeddings.embedQuery("pizza"),
    embeddings.embedQuery("Japan"),
    embeddings.embedQuery("sushi"),
  ]);

  // Perform vector arithmetic: pizza - Italy + Japan
  const pizzaMinusItaly = subtractVectors(pizzaEmbed, italyEmbed);
  const result2 = addVectors(pizzaMinusItaly, japanEmbed);

  // Calculate similarity with sushi
  const similarityToSushi = cosineSimilarity(result2, sushiEmbed);

  console.log(`‚úÖ Similarity to 'sushi': ${(similarityToSushi * 100).toFixed(2)}%`);
  console.log("\nWhat this means:");
  console.log("  ‚Ä¢ Pizza is to Italy as sushi is to Japan");
  console.log("  ‚Ä¢ The embeddings understand cultural food associations");
  console.log("  ‚Ä¢ Subtracting 'Italy' removes the country, adding 'Japan' finds Japan's iconic food");

  // Show comparison with unrelated food
  const burgerEmbed = await embeddings.embedQuery("hamburger");
  const similarityToBurger = cosineSimilarity(result2, burgerEmbed);

  console.log(`\nüìä Comparison: Similarity to 'hamburger': ${(similarityToBurger * 100).toFixed(2)}%`);
  console.log(`   (Lower than sushi, as expected - hamburger is more associated with USA)\n`);

  console.log("=".repeat(70) + "\n");

  // ============================================================================
  // Example 3: Synonym Clustering
  // Demonstrating: Similar words have similar embeddings
  // ============================================================================

  console.log("üòä Example 3: Synonym Clustering");
  console.log("‚îÄ".repeat(70));
  console.log("\nTesting similarity between synonyms:\n");

  // Generate embeddings for synonyms
  const [happyEmbed, joyfulEmbed, cheerfulEmbed, sadEmbed] = await Promise.all([
    embeddings.embedQuery("happy"),
    embeddings.embedQuery("joyful"),
    embeddings.embedQuery("cheerful"),
    embeddings.embedQuery("sad"),
  ]);

  // Calculate similarities
  const happyJoyful = cosineSimilarity(happyEmbed, joyfulEmbed);
  const happyCheerful = cosineSimilarity(happyEmbed, cheerfulEmbed);
  const happySad = cosineSimilarity(happyEmbed, sadEmbed);

  console.log(`Similarity: 'happy' ‚Üî 'joyful'   = ${(happyJoyful * 100).toFixed(2)}% ‚úÖ (synonyms)`);
  console.log(`Similarity: 'happy' ‚Üî 'cheerful' = ${(happyCheerful * 100).toFixed(2)}% ‚úÖ (synonyms)`);
  console.log(`Similarity: 'happy' ‚Üî 'sad'      = ${(happySad * 100).toFixed(2)}% ‚ùå (opposites)`);

  console.log("\nWhat this means:");
  console.log("  ‚Ä¢ Words with similar meanings have similar embeddings");
  console.log("  ‚Ä¢ They cluster close together in vector space");
  console.log("  ‚Ä¢ Opposite meanings have lower similarity scores");

  console.log("\n" + "=".repeat(70) + "\n");

  // ============================================================================
  // Summary
  // ============================================================================

  console.log("üéì Key Takeaways:");
  console.log("‚îÄ".repeat(70));
  console.log("\n1. Embeddings capture semantic relationships:");
  console.log("   ‚Ä¢ Paris:France :: Rome:Italy (capital cities)");
  console.log("   ‚Ä¢ pizza:Italy :: sushi:Japan (cultural foods)");
  console.log("");
  console.log("2. Vector arithmetic works on meanings:");
  console.log("   ‚Ä¢ Adding/subtracting embeddings preserves relationships");
  console.log("   ‚Ä¢ The math 'understands' concepts like country, capital, food");
  console.log("");
  console.log("3. Synonyms cluster together:");
  console.log("   ‚Ä¢ Similar meanings = nearby in vector space");
  console.log("   ‚Ä¢ Different meanings = farther apart");
  console.log("");
  console.log("4. This enables powerful applications:");
  console.log("   ‚Ä¢ Semantic search (find similar meanings, not just keywords)");
  console.log("   ‚Ä¢ Analogy completion (A:B :: C:?)");
  console.log("   ‚Ä¢ Document clustering (group by topic)");
  console.log("   ‚Ä¢ Recommendation systems (find similar items)");
  console.log("\n‚úÖ These relationships emerge from training on massive text corpora!");
}

main().catch(console.error);
