/**
 * Model Comparison
 * Run: npx tsx 01-introduction/code/03-model-comparison.ts
 *
 * ü§ñ Try asking GitHub Copilot Chat (https://github.com/features/copilot):
 * - "Why do we create a new ChatOpenAI instance inside the loop?"
 * - "How can I add another model to this comparison?"
 * - "What other metrics besides time could I measure for each model?"
 */

import { ChatOpenAI } from "@langchain/openai";
import "dotenv/config";

async function compareModels() {
  console.log("üî¨ Comparing AI Models\n");

  const prompt = "Explain recursion in programming in one sentence.";
  const models = ["gpt-5", "gpt-5-mini"];

  for (const modelName of models) {
    console.log(`\nüìä Testing: ${modelName}`);
    console.log("‚îÄ".repeat(50));

    // Override the model for this test
    const model = new ChatOpenAI({
      model: modelName,
      configuration: { baseURL: process.env.AI_ENDPOINT },
      apiKey: process.env.AI_API_KEY,
    });

    const startTime = Date.now();
    const response = await model.invoke(prompt);
    const duration = Date.now() - startTime;

    console.log(`Response: ${response.content}`);
    console.log(`‚è±Ô∏è  Time: ${duration}ms`);
  }

  console.log("\n‚úÖ Comparison complete!");
  console.log("\nüí° Key Observations:");
  console.log("   - gpt-5 is more capable and detailed");
  console.log("   - gpt-5-mini is faster and uses fewer resources");
  console.log("   - Choose based on your needs: speed vs. capability");
}

compareModels().catch(console.error);
