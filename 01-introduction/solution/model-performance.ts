/**
 * Challenge 4 Solution: Model Performance Comparison
 *
 * Compare different models on speed, response length, and quality.
 */

import { ChatOpenAI } from "@langchain/openai";
import "dotenv/config";

const question = "Explain the difference between machine learning and deep learning.";

const models = [
  { name: "gpt-4o", description: "Most capable" },
  { name: "gpt-4o-mini", description: "Fast and efficient" },
];

interface ModelResult {
  name: string;
  time: number;
  length: number;
  response: string;
}

async function testModel(modelName: string): Promise<ModelResult> {
  const model = new ChatOpenAI({
    model: modelName,
    configuration: {
      baseURL: "https://models.inference.ai.azure.com",
    },
    apiKey: process.env.GITHUB_TOKEN,
  });

  const startTime = Date.now();
  const response = await model.invoke(question);
  const endTime = Date.now();

  return {
    name: modelName,
    time: endTime - startTime,
    length: response.content.toString().length,
    response: response.content.toString(),
  };
}

function getQualityStars(length: number): string {
  // Simple heuristic: longer responses might be more detailed
  if (length > 400) return "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê";
  if (length > 300) return "‚≠ê‚≠ê‚≠ê‚≠ê";
  if (length > 200) return "‚≠ê‚≠ê‚≠ê";
  return "‚≠ê‚≠ê";
}

async function compareModels() {
  console.log("üî¨ Model Performance Comparison\n");
  console.log(`Question: "${question}"\n`);
  console.log("Testing models...\n");

  const results: ModelResult[] = [];

  for (const modelInfo of models) {
    console.log(`Testing ${modelInfo.name}...`);
    const result = await testModel(modelInfo.name);
    results.push(result);

    // Small delay to avoid rate limits
    await new Promise((resolve) => setTimeout(resolve, 1000));
  }

  // Display results table
  console.log("\n" + "=".repeat(80));
  console.log("üìä Results");
  console.log("=".repeat(80));
  console.log(
    `${"Model".padEnd(15)} | ${"Time".padEnd(8)} | ${"Length".padEnd(8)} | ${"Quality".padEnd(10)}`
  );
  console.log("-".repeat(80));

  for (const result of results) {
    const time = `${result.time}ms`;
    const length = `${result.length}ch`;
    const quality = getQualityStars(result.length);

    console.log(
      `${result.name.padEnd(15)} | ${time.padEnd(8)} | ${length.padEnd(8)} | ${quality.padEnd(10)}`
    );
  }

  console.log("=".repeat(80));

  // Show full responses
  console.log("\nüìù Full Responses:\n");

  for (const result of results) {
    console.log(`\n${result.name}`);
    console.log("-".repeat(80));
    console.log(result.response);
    console.log("\n");
  }

  // Analysis
  console.log("=".repeat(80));
  console.log("üí° Analysis:");
  console.log("=".repeat(80));

  const fastest = results.reduce((prev, curr) => (curr.time < prev.time ? curr : prev));
  const longest = results.reduce((prev, curr) => (curr.length > prev.length ? curr : prev));

  console.log(`‚ö° Fastest: ${fastest.name} (${fastest.time}ms)`);
  console.log(`üìù Most detailed: ${longest.name} (${longest.length} characters)`);
  console.log("\nüéØ Recommendations:");
  console.log("   - Use gpt-4o for complex tasks needing detailed responses");
  console.log("   - Use gpt-4o-mini for quick responses and cost efficiency");
}

compareModels().catch(console.error);
